\section{Conclusion}

Our implementation demonstrates how HMMs can be implemented using a
graph-parallel approach.  We believe that this project is a success, because it provides a starting point for others to parallelize this nontrivial machine learning algorithm in a framework that's gaining traction fast.   We intend to share our work with Dato, for example, since they already provide GraphLab users with a host of other machine learning algorithms and preprocessing routines, but not HMMs.  Even though our graph-parallel approach is slower than
our serial approach, we show that it achieves better scaling than the serial
approach in many cases. We conjecture that the difference in runtime relates to GraphLab's communication cost.  We also believe that GraphLab's immutable graph data structure, $\tt{SGraph}$ may be slow to manipulate and may be copied often.  Finally, we  believe that on some $\tt{triple_apply}$ calls, the core code may be unnecessarily locking the vertex data, since the locking is performed even when vertex data is not mutated, but simply read.   We have suggested this optimization to the Dato team and they have endorsed this suggestion as an area for future improvement in their product, encouraging us to submit a pull request to their open source core code.  All put together, we are very optimistic about the work we have done and were glad to get our feet wet with a framework that we were previously less-than-comfortable with.
\section*{Acknowledgement}

The authors like to thank the teaching staff of CS 205, as well as Jay Gu and
Srikrishna Sridhar from Dato, for their tips and helpful advice.
