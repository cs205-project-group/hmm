\section{Background}

\subsection{Hidden Markov Model}
A hidden Markov model assumes that the system follows a Markov process. For this paper, we will assume a finite number of states and observations. The probability of transitioning from state $i$ to state $j$ only depends on the current state, and we define transition matrix $A$ such that element $A_{ij}$ is the probability  of transitioning from $i$ to $j$. In an HMM, we cannot observe states, but we can observe outputs generated by the states. Given $N$ states and $M$ observations, we capture this with a $N \times M$ emission matrix $B$, where $B_{ij}$ is the probability of observation $j$ given the state is $i$. Finally, we define a $N$ length vector as $\pi$ as the prior, where $\pi_i$ is the probability of initial state being $i$. 

We can thus capture all the information about an HMM with $\theta = (A, B, \pi)$. In an HMM, we are given a sequence of $T$ observations, and we will use a series of forward and backwards propagations to estimate the parameters $\theta = (A, B, \pi)$.

\subsection{Baum-Welch}
Let $(X_1, X_2, \ldots, X_N)$ denote the $N$ states and $\{O_1, O_2, \ldots, O_T\}$ the $T$ observed outputs. the Baum-Welch uses expectation-maximization to find the maximum likelihood estimate.

We set $\theta  =  (A, B, \pi)$ randomly. For the forward procedure, we define $\alpha_i(t) = P(O_1 = o_t, \ldots, O_t = o_t | \theta)$. That is, the probability of observing the first $t$ outputs given a set of parameters. We can then find $\alpha_i(t+1) =  $


